{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Represent learning... \n",
    "\n",
    "* "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mendenhall’s Characteristic Curves of Composition are a graphical representation of the distribution of word lengths in written language.\n",
    "\n",
    "In 1887, the American statistician Edward Mendenhall analyzed the length of words in several English-language texts, including the plays of Shakespeare and the Bible. He found that the distribution of word lengths followed a characteristic curve that was roughly bell-shaped.\n",
    "\n",
    "Mendenhall's characteristic curves show the percentage of words in a given text that have a certain length. For example, the curve might show that 5% of the words in the text have one letter, 15% have two letters, 30% have three letters, and so on. The curve peaks at the most common word length, which tends to be around five or six letters in English.\n",
    "\n",
    "Mendenhall's work was significant because it provided a quantitative way to describe the structure of language. His characteristic curves have been used in a variety of fields, including linguistics, psychology, and information science. They are also still used today to analyze text data in fields such as natural language processing and computational linguistics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kilgariff's Chi-Squared Method is a statistical technique used in natural language processing to identify significant associations between words or phrases in a corpus of text.\n",
    "\n",
    "The method is named after Adam Kilgariff, a computational linguist who developed it in the late 1990s. The Chi-Squared Method involves calculating a Chi-Squared statistic for each word or phrase in a corpus, which measures the difference between the observed frequency of the word or phrase and the expected frequency based on chance alone.\n",
    "\n",
    "To apply the method, the corpus is first segmented into a set of phrases or n-grams. For each phrase, the observed frequency of the phrase is calculated by counting the number of times it appears in the corpus. The expected frequency is then calculated based on the assumption that the occurrence of the phrase is independent of the occurrence of other phrases in the corpus.\n",
    "\n",
    "The Chi-Squared statistic is then calculated using the formula:\n",
    "\n",
    "χ2 = (O - E)^2 / E\n",
    "\n",
    "where O is the observed frequency of the phrase, and E is the expected frequency. This measures the degree of association between the phrase and the rest of the corpus. A low Chi-Squared value indicates a strong association, while a high value indicates a weak association.\n",
    "\n",
    "Kilgariff's Chi-Squared Method has been used in a variety of natural language processing tasks, including text classification, information retrieval, and automatic term extraction. It is a useful tool for identifying significant collocations or phrase pairs in a corpus, which can provide insights into the underlying structure and meaning of the language.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John Burrows' Delta Method is a statistical technique used in stylometry and authorship attribution to compare the language usage patterns of different texts.\n",
    "\n",
    "The Delta Method involves calculating a statistical distance measure between two or more sets of linguistic features, such as word frequencies, grammatical constructions, or lexical diversity. The distance measure is based on the difference between the observed frequencies of the features in each text and the expected frequencies based on a reference corpus or baseline.\n",
    "\n",
    "The Delta Method uses a variant of the Z-score statistic, which is calculated as the difference between the observed and expected frequencies divided by the standard deviation of the expected frequencies. The resulting Z-scores are then normalized by taking the absolute value and transforming them to a standard normal distribution using a probability density function.\n",
    "\n",
    "The distance between two texts is then calculated as the Euclidean distance between the normalized Z-score vectors for each linguistic feature. This measures the degree of similarity or difference between the two texts in terms of their language usage patterns.\n",
    "\n",
    "Burrows' Delta Method has been used in a variety of applications, including authorship attribution, genre classification, and text similarity analysis. It provides a powerful tool for identifying subtle differences in writing style and detecting instances of plagiarism or forgery. However, it requires careful selection and normalization of linguistic features to avoid bias and ensure accurate comparisons."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

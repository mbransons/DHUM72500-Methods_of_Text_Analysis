{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/week7_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAkti4G4I1dy"
      },
      "source": [
        "# Week 7: Jupyter Notebook Assignment - Working with Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbVLjq4pI1d0"
      },
      "source": [
        "Fill out the cell below with your information. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kdnDlnxFI1d0"
      },
      "source": [
        "* Student Name: Michael Smith\n",
        "* Date: 3/20/23\n",
        "* Instructor: Lisa Rhody\n",
        "* Assignment due: \n",
        "* Methods of Text Analysis\n",
        "* MA in DH at The Graduate Center, CUNY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncgW8rT_I1d0"
      },
      "source": [
        "## Objectives\n",
        "The purpose of this notebook is to get some hands-on experience putting what you've seen in tutorials about importing and working with text in Python into practice. You'll also be asked to put the reading you've been doing all semester into conversation with the process of importing, cleaning, and preparing data. \n",
        "\n",
        "The object of the notebooks this week is: \n",
        "* To practice several ways of importing text into your Python environment to study; \n",
        "* To become more familiar with various pipelines for cleaning and preparing data for text analysis; \n",
        "* To consider the challenges that the availability and scarcity of data presents to the literary scholar (and to consider how other kinds of research might also need to address similar issues); \n",
        "* To connect examples of real-world text analysis projects with the practical process of cleaning and preparing data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egrAMMlOMIBS"
      },
      "source": [
        "# Getting Started\n",
        "We're going to start by importing some important libraries for working with text data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yC6MGOIjJo9s"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtTLIs5OI1d0"
      },
      "source": [
        "# Importing Data\n",
        "So far, we have worked with data during the Datacamp exercises, but that was a much more controlled environment. When you are actually doing your own text analysis project, you will have a much messier process. During this week's reading, you will have read several pieces about what cleaning takes place and some of the challenges that data presents when working with text. In particular, we're looking at text analysis from a humanities / litereary perspective; however, one might argue that these challenges are more similar to the text analysis one might perform in the social sciences or with non-fiction work than might appear to be the case on the surface. \n",
        "\n",
        "In this lesson, we'll practice importing data: \n",
        "* from a file already on your computer (using a directory path); \n",
        "* from a file on the web using a URL request \n",
        "* from a file on the web using Beautiful Soup. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8X-8rGZI1d1"
      },
      "source": [
        "### Loading data from a flat file on your local computer\n",
        "Before you get started, be sure to download this file onto your local computer and save it as herland.txt. \n",
        "\n",
        "Next, we're going to import `herland.txt` using an upload function that is part of the google.colab Python package. This function will open a button under the cell that you can use to \"Choose Files\" from your local computer. Choose the `herland.txt` file and then upload it. The for loop below will print out what the name of the file is that you are saving to the Google Colab content folder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "437GlwvvJsnT"
      },
      "outputs": [],
      "source": [
        "# Running from a locally hosted notebook so the upload of the text is unnecessary\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print(\"User uploaded file '{name} with length {length} bytes\".format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlKkEPMIOO6j"
      },
      "source": [
        "To find the file you just uploaded, look to the left side of this browser window. Click on the icon of a file folder. A directory structure should open. Click on the arrow next to `content` and you should see your uploaded file appear inside. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoGcEOF_NTO1"
      },
      "source": [
        "Then we're going to use a Python function `open()`. We'll use a `for` loop, which simply means that we'll do an action that repeats until we tell it to stop. The following code says that we want to `open` the file `herland.txt` so we can read it (argument `mode='r'`). Then we're going to close the file. When we do this, we're going to assign a variable name to the resulting data, which is now a string called `file`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8C498LV7I1d1"
      },
      "outputs": [],
      "source": [
        "filename = 'herland.txt'\n",
        "herland = open(filename, mode='r')\n",
        "hertext = herland.read()\n",
        "herland.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j__79ZoL6TK"
      },
      "source": [
        "Another way to read the text from a file into Python is to use a \"context manager.\" The following tells python that with the `herland.txt` file open, read in the text and create a variable called `file` to store the data. Then, the next line tells Python to print the new variable `file`. When you run the next cell, it is going to print out the entire text of *Herland*. That's a lot of text, so once you've done it, you can clear the cell's output and move on to the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gIaGplApI1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Herland, by Charlotte Perkins Stetson Gilman\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "Title: Herland\n",
            "\n",
            "Author: Charlotte Perkins Stetson Gilman\n",
            "\n",
            "Posting Date: June 25, 2008 [EBook #32]\n",
            "Release Date: May 10, 1992\n",
            "Last Updated: October 14, 2016\n",
            "\n",
            "Language: Engl\n"
          ]
        }
      ],
      "source": [
        "# Here is how you print a string from a file without having to close the file using a context manager\n",
        "\n",
        "with open('herland.txt','r') as file:\n",
        "    print(file.read()[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jWCXVgjYI1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Herland, by Charlotte Perkins Stetson Gilman\n",
            "\n",
            "\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# If you don't want to save the text of the file, but just want to peek into it to see what's there, you could use this method. \n",
        "\n",
        "with open('herland.txt') as file:\n",
        "    print(file.readline())\n",
        "    print(file.readline())\n",
        "    print(file.readline())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AE2MisyI1d3"
      },
      "source": [
        "### What happens when you import a flat file? \n",
        "\n",
        "The python function `type()` will return to you output that explains the data type you are working with. When you pass the new text object `herland` through the `type()` function below, what response do you get? The response will look different from other data types that you've used before. In this case, it is read in as a \"file object.\" Remember that Python won't know how to handle data unless it fits a particular data type that the computer expects when passing a function to it. In the next input, we ask Python for the length of the file. This will throw an error. Why do you think that is? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dv-e2lDBI1d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_io.TextIOWrapper"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# herland is a file object, not a string. \n",
        "type(herland)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XZ5wcOL1I1d3",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "315999"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# since herland is a file object and not a string, you can't find the length of it.\n",
        "len(hertext)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gNMI0VRCI1d3"
      },
      "source": [
        "#### Response here: 'herland' is presently an I/O text stream object read from the 'herland.txt' file. In order to return the length of the text in the stream it must first be converted to a string using the .read() method. This was done with hertext = herland.read(). The length of hertext can be found using the len() function with 315,999 characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-GfT0Q9I1d3"
      },
      "source": [
        "We had to go through a process to convert the file object to a string. \n",
        "\n",
        "Looking at the cells below, which variable should return `type()` as a string? (The answer is in the cell below.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VLjQ9TP2I1d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# but hertext is a different datatype. How would you check? \n",
        "type(hertext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDiIVyKI1d4"
      },
      "source": [
        "Once you have a string, there are a number of functions that you can make use of. One of those is the `len()` command, which you can run below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "u6v1YjKYI1d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "315999"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How many characters are in the hertext string? \n",
        "len(hertext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqfZqn1rI1d4"
      },
      "source": [
        "Once an object is recognized as a string, you can begin manipulating it. For example, you could count the number of times the sequence of characters \"her\" appear within the entire text of _Herland_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4vFsxjeLI1d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1244"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hertext.count('her', 0, -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSohRXHiI1d4"
      },
      "source": [
        "The ability to count characters, words, n-grams, etc. means that we can also more easily target specific sections of the text. For example, when you print to your screen the opening of the herland file, you notice that it is accompanied with metadata. For the purposes of text analysis, what would be the advantages or disadvantages of removing the metadata associated with _Herland_?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project Guttenberg text files include a metadata heading at the start of every text file. For text analysis we may wish to exclude this heading as part of word studies as it description of the text, not the text itself. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OgBINGAqI1d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Herland, by Charlotte Perkins Stetson Gilman\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Project Gutenberg License included\n",
            "with this eBook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "Title: Herland\n",
            "\n",
            "Author: Charlotte Perkins Stetson Gilman\n",
            "\n",
            "Posting Date: June 25, 2008 [EBook #32]\n",
            "Release Date: May 10, 1992\n",
            "Last Updated: October 14, 2016\n",
            "\n",
            "Language: Engl\n"
          ]
        }
      ],
      "source": [
        "# What is happening at the beginning of the herland.txt file, though? We can check to see by using an index. \n",
        "print(hertext[:500])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a python library for removing the gutenberg header and footer metadata. Below is the installation and import process with a sample from 'hertext' with removed header and footer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gutenberg-cleaner in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (0.1.6)\n",
            "Requirement already satisfied: nltk in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (from gutenberg-cleaner) (3.7)\n",
            "Requirement already satisfied: joblib in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (from nltk->gutenberg-cleaner) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (from nltk->gutenberg-cleaner) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (from nltk->gutenberg-cleaner) (2022.7.9)\n",
            "Requirement already satisfied: click in /Users/michaelsmith/opt/anaconda3/lib/python3.9/site-packages (from nltk->gutenberg-cleaner) (8.0.4)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install gutenberg-cleaner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gutenberg_cleaner import simple_cleaner, super_cleaner\n",
        "\n",
        "herclean = simple_cleaner(hertext)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHERLAND\\n\\nby Charlotte Perkins Stetson Gilman\\n\\n\\n\\n\\nCHAPTER 1. A Not Unnatural Enterprise\\n\\n\\nThis is written from memory, unfortunately. If I could have brought with\\nme the material I so carefully prepared, this would be a very different\\nstory. Whole books full of notes, carefully copied records, firsthand\\ndescriptions, and the pictures--that’s the worst loss. We had some\\nbird’s-eyes of the cities and parks; a lot of lovely views of streets,\\nof buildings, outside and in, and some of those '"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "herclean[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79eRBN2mI1d5"
      },
      "source": [
        "Working with a string is *more* helpful than simply working with a text object, but there are other things that we can do to the text to make it more easily manipulated in Python and NLTK. For example, when you're working with a string, it's not easy to count whole words. The NLTK word tokenizer function, however, will take a string and turn it into \"tokens\"--discrete segments of characters. Tokenized strings become a new data type--a list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KZDfyI-mI1d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hertokens = nltk.word_tokenize(herclean)\n",
        "type(hertokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2LRs2lI1d5"
      },
      "source": [
        "A tokenized list can be called, acted upon, and manipulated differently than a string. If we call just the tokens that are in index positions 0-15, here is what you would get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xvB1XxkbI1d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['HERLAND',\n",
              " 'by',\n",
              " 'Charlotte',\n",
              " 'Perkins',\n",
              " 'Stetson',\n",
              " 'Gilman',\n",
              " 'CHAPTER',\n",
              " '1',\n",
              " '.',\n",
              " 'A',\n",
              " 'Not',\n",
              " 'Unnatural',\n",
              " 'Enterprise',\n",
              " 'This',\n",
              " 'is']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hertokens[:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s4ySkAL5I1d5"
      },
      "outputs": [],
      "source": [
        "text1 = nltk.Text(hertokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_bPbpcdTI1d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "nltk.text.Text"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j97hxS_pI1d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "65090"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(text1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6N5_2fkQI1d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['for',\n",
              " 'weeks',\n",
              " 'past',\n",
              " ',',\n",
              " 'the',\n",
              " 'same',\n",
              " 'taste',\n",
              " '.',\n",
              " 'I',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'speak',\n",
              " 'of',\n",
              " 'that',\n",
              " 'river',\n",
              " 'to',\n",
              " 'our',\n",
              " 'last',\n",
              " 'guide',\n",
              " ',',\n",
              " 'a',\n",
              " 'rather',\n",
              " 'superior',\n",
              " 'fellow',\n",
              " 'with']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text1[1000:1025]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7OVaudI1d6"
      },
      "source": [
        "### Review\n",
        "When you import text from a flat file that is saved on your local computer, what will you need to do in order to select parts of the text using an index? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An indexable list can only be used after a single text string as tokenized into a list of strings. The tokenizer uses the spaces and punctuation as a delimiter to separate each portion of the string. After we have a list index values can be used to select a single string in the list or ranges of the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UAirNZpI1d6"
      },
      "source": [
        "## Ingesting data from a URL\n",
        "\n",
        "Next, we're going to retrieve text directly from a URL with the `urlllib` package\n",
        "To do this, we're going to call the package `urllib` and specifically from that we're going to use `urlretrieve.` Next, we need to assign the text in the file to a variable. In this case, that variable is `url`. We're going to run `urlretrieve` with two parameters, the name of the URL you want to import (which you assigned to the variable `url` above, and the file name and extension. Here that is `203-0.txt.` If you pay attention to the output, you'll realize that you've imported the file as an object. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-34iqZqeSJXF"
      },
      "outputs": [],
      "source": [
        "from urllib import request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# I was having issues with the suggested pattern returning a value so am using the suggested pattern\n",
        "# from https://stackoverflow.com/questions/61897926/project-gutenberg-accessing-text-with-url\n",
        "url = \"https://www.gutenberg.org/files/203/203-0.txt\"\n",
        "\n",
        "response = request.urlopen(url)\n",
        "raw = response.read()\n",
        "uncletom = raw.decode(\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQplVwFI1d6"
      },
      "source": [
        "### Question:\n",
        "Using what you've learned so far, how would you figure out what data type the file `uncletom` is? Add a cell below and show how you would find the answer. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the pattern updated to include a read() and decode() of the url stream the file type is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iedWrfJhTTRO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(uncletom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNo4vVZKUht1"
      },
      "source": [
        "Next, we're going to turn the text of Uncle Tom's Cabin into a list. A list is a mutable, ordered sequence of items. It can be indexed, sliced, and changed. Items in the list can be accessed through it's indexical placement. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GbrWDSiHUOiH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "180925"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# first remove headers and footers of Guttenberg text\n",
        "uncleclean = simple_cleaner(uncletom)\n",
        "words = uncleclean.split()\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "LZtxrM6OUZ6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Y-w18bVQuo"
      },
      "source": [
        "Let's practice those steps again, but with a new file this time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iXxWF3eMVi4M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "shakespeare = 'http://composingprograms.com/shakespeare.txt'\n",
        "\n",
        "print( type(shakespeare) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pSUtnSJhWAgH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "shakespeare = 'http://composingprograms.com/shakespeare.txt'\n",
        "# shakespeare = urlopen('http://composingprograms.com/shakespeare.txt')\n",
        "response = request.urlopen(shakespeare)\n",
        "raw = response.read()\n",
        "shakespeare = raw.decode(\"utf-8-sig\")\n",
        "\n",
        "print(type(shakespeare))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5XE2QAmWWYh2"
      },
      "outputs": [],
      "source": [
        "# dir(shakespeare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5dv-DfiLWdI_"
      },
      "outputs": [],
      "source": [
        "words = shakespeare.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6bVdrVC3WjrW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JLNGe1QyXPQA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A', \"MIDSUMMER-NIGHT'S\", 'DREAM']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "title = words[0:3]\n",
        "title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_tD1S3TWXSxU"
      },
      "outputs": [],
      "source": [
        "body = words[3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5ECbMdt8XW8G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour', 'Draws', 'on']\n"
          ]
        }
      ],
      "source": [
        "print(body[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ANoBskW4_l"
      },
      "source": [
        "\n",
        "__Indexing Operator__\n",
        "\n",
        "Indexing operator ([ ]) selects one or more elements from a sequence. Each element of a sequence is assigned a number - its position or index. Index must be an integer value and is called inside a pair of square brackets. \n",
        "\n",
        "The operation that extracts a subsequence is called __slicing__. When selecting more than one element __\": operator\"__ is used with integer before and after it to indicate where to start and where to stop the index, respectively.\n",
        "\n",
        "Python indexing starts at 0 and ends at (n-1), where n refers to the number of items in the sequence. The function \"len\" can be used to get the number of items in a list. \n",
        "\n",
        "Negative indexing is also supported by Python. It can be done by adding \"-\" operator before the integer value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "b5-zzKQkWnTd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "980634\n"
          ]
        }
      ],
      "source": [
        "n_words = len(body)\n",
        "print( n_words )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "4EBY3CBPXbkz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n"
          ]
        }
      ],
      "source": [
        "# The index value is out of range as the first index value is 0 and the final index value is the length - 1\n",
        "# print( body[980634])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "44vtxFDXXnYg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".\n"
          ]
        }
      ],
      "source": [
        "print( body[980633])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h6vTMIntXrwS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour', 'Draws', 'on']\n"
          ]
        }
      ],
      "source": [
        "sub_body = body[:10]\n",
        "print( sub_body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "z_Wx7eRzXwIU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Now', ',', 'fair', 'Hippolyta', ',', 'our', 'nuptial', 'hour']\n"
          ]
        }
      ],
      "source": [
        "print( sub_body[:-2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IoQu8aM1Xzgi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Now', 'fair', ',', 'nuptial', 'Draws']\n"
          ]
        }
      ],
      "source": [
        "print( sub_body[::2])    # gives every 2nd element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2JrMe2UYHbL"
      },
      "source": [
        "__Python Syntax__\n",
        "\n",
        "Syntax refers to the structure of the language. \n",
        "\n",
        "The end of the statement does not require semicolon or other symbol. After a statement is complete, the code is considered completed. However, using semicolon can allow you to execute two separate codes from the same line. \n",
        "\n",
        "Indentation i.e. the whitespace matters in Python. A block of code is a set of statements that should be treated as a unit even when written in a new line. A code block in python are denoted by indentation. For example, in compound statements such as loops and conditionals, after the colon we must enter into a new line and add exactly four spaces to continue further. Whitespaces __within__ the same line does not matter however.  \n",
        "\n",
        "Comments about codes can be made using hashtag #. anything written after # is ignored by the interpreter. Python does not have any syntax for multi-line comments. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "5TK22YcjYPsO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['now', ',', 'fair', 'hippolyta', ',', 'our', 'nuptial', 'hour', 'draws', 'on']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sub_body_lowercase = []\n",
        "for word in sub_body:\n",
        "  sub_body_lowercase.append(word.lower())\n",
        "  #print(sub_body_lowercase)\n",
        "#print(sub_body_lowercase)\n",
        "sub_body_lowercase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksKMYa0vI1d7"
      },
      "source": [
        "## Importing an HTML file using an http: request\n",
        "The previous two files that we imported were _plain text_ files. In other words, there is little to no descriptive encoding. However, we can also use another module from the URLLIB package that is designed to import an .html file directly from the web. We can actually do this with just a few lines of code. First, we import the URLLIB package, and specifically the `request` module. We assign the URL we want to manipulate by assigning the URL to a variable. Next, we pass the URL through the urlopen.request function from the URLLIB package, and also at the same time \"read\" the file. The output of that string becomes the variable `html`. When we print the variable html, we discover that all of the HTML from the page has been pulled into the variable name. Unfortuantely, it doesn't look very clean. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xknGfuL7I1d7"
      },
      "outputs": [],
      "source": [
        "# Now import the bibliography page from Colored Conventions in HTML\n",
        "import urllib.request\n",
        "anotherurl='http://coloredconventions.org/exhibits/show/bishophmturner'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SufMVoVhI1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'<!DOCTYPE html>\\n<!--[if IE 6]>\\n<html id=\"ie6\" lang=\"en-US\" xmlns:fb=\"https://www.facebook.com/2008/fbml\" xmlns:addthis=\"https://www.addthis.com/help/api-spec\" >\\n<![endif]-->\\n<!--[if IE 7]>\\n<html id=\"ie7\" lang=\"en-US\" xmlns:fb=\"https://www.facebook.com/2008/fbml\" xmlns:addthis=\"https://www.addthis.com/help/api-spec\" >\\n<![endif]-->\\n<!--[if IE 8]>\\n<html id=\"ie8\" lang=\"en-US\" xmlns:fb=\"https://www.facebook.com/2008/fbml\" xmlns:addthis=\"https://www.addthis.com/help/api-spec\" >\\n<![endif]-->\\n<!--[if !('\n"
          ]
        }
      ],
      "source": [
        "html = urllib.request.urlopen(anotherurl).read()\n",
        "print(html[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi1Cyb-SI1d7"
      },
      "source": [
        "If you are interested in doing text analysis of a webpage, and the only way to ingest the web page is with HTML included, what are things you might need to learn to do to separate the HTML tags from the text? Look at the code above and write a short description of what might need to stay and what might need to be extracted. Should the extracted data be preserved or discarded? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Webscraping projects require looking at the structure of the HTML document and find the particular pieces that you're interested in parsing. On this page there is a div element with the class=\"et_pb_text_inner\" which contains the entirety of the credits portion on the webpage. If you can select this particular element by its class (assuming it's unique) and then strip HTML tags that wrap the text, you would be succesful at getting this data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iUkeiKmI1d7"
      },
      "source": [
        "# Importing Data by Webscraping with BeautifulSoup\n",
        "If you are interested in scraping data from the open web, BeautifulSoup is a Python pacakge worth exploring in detail. For our purposes here, though, we're going to consider how to use Beautiful Soup to turn \"unstructured\" data into \"structured\" data. As you read through this section, consider Muñoz and Rawson's argument about data cleaning. Is there a need for the data to stay unstructured? What is the value of cleaning? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "u6TpNC7TI1d7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "h80IcFswI1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\" xmlns:addthis=\"https://www.addthis.com/help/api-spec\" xmlns:fb=\"https://www.facebook.com/2008/fbml\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
            "  <link href=\"https://coloredconventions.org/xmlrpc.php\" rel=\"pingback\"/>\n",
            "  <script type=\"text/javascript\">\n",
            "   document.documentElement.className = 'js';\n",
            "  </script>\n",
            "  <link crossorigin=\"\" href=\"https://fonts.gstatic.com\" rel=\"preconnect\"/>\n",
            "  <style id=\"et-builder-googlefonts\n"
          ]
        }
      ],
      "source": [
        "# Specify url: url\n",
        "url4 = 'http://coloredconventions.org/press#scholarship'\n",
        "\n",
        "# Package the request, send the request and catch the response: r\n",
        "r = requests.get(url4)\n",
        "\n",
        "# Extracts the response as html: html_doc\n",
        "html_doc = r.text\n",
        "\n",
        "# Create a BeautifulSoup object from the HTML: soup\n",
        "soup = BeautifulSoup(html_doc)\n",
        "\n",
        "# Prettify the BeautifulSoup object: pretty_soup\n",
        "pretty_soup = soup.prettify()\n",
        "\n",
        "# Print the response\n",
        "print(pretty_soup[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-mNbfsI1d8"
      },
      "source": [
        "Compare the text imported using the \"webscraping\" method included with BeautifulSoup versus the option of importing the entire file using URLLIB. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k5U0rkAI1d8"
      },
      "source": [
        "## Cleaning up Webscraped text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Chi4_KhUI1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<title>Press &amp; Notices - Colored Conventions Project</title>\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify url: url\n",
        "url5 = 'http://coloredconventions.org/press#scholarship'\n",
        "\n",
        "# Package the request, send the request and catch the response: r\n",
        "r = requests.get(url5)\n",
        "\n",
        "# Extract the response as html: html_doc\n",
        "html_doc = r.text\n",
        "\n",
        "# Create a BeautifulSoup object from the HTML: soup\n",
        "soup = BeautifulSoup(html_doc)\n",
        "\n",
        "# Get the title of Colored Conventions' webpage: ccc_title\n",
        "ccc_title = soup.title\n",
        "\n",
        "# Print the title of Colored Conventions' webpage to the shell\n",
        "print(ccc_title)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "LRql1zrYI1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Press & Notices\n",
            "Academic Journals\n",
            "Fagan, Benjamin. “Chronicling White America.” American Periodicals: A Journal of History & Criticism 26, no. 1 (2016): 10–13.\n",
            "Spires, Derrick R. “The Captive Stage: Performance and the Proslavery Imagination of the Antebellum North by Douglas A. Jones (review).” Early American Literature 51, no. 1 (2016): 200–205.\n",
            "Eric Gardner. and Joycelyn Moody. “Introduction: Black Periodical Studies.” American Periodicals: A Journal of History, Criticism, and Bibliography 25.2 (2015): 105-111. Project MUSE. Web.\n",
            "Joycelyn Moody. and Howard Rambsy II. “Guest Editors’ Introduction: African American Print Cultures.”MELUS: Multi-Ethnic Literature of the U.S. 40.3 (2015): 1-11. Project MUSE. Web.\n",
            "Roundtable: The Colored Conventions Project, Fall 2015. \n",
            "The Colored Conventions Project and the Changing Same, by P. Gabrielle Foreman\n",
            "Toward Meaning-making in the Digital Age: Black Women, Black Data and Colored Conventions, by Sarah Patterson\n",
            "The Colored Conventions Movement in Print and Beyond, by Curtis Small\n",
            "Convention Minutes and Unconventional Proceedings, by Jim Casey\n",
            "Liberating History: Reflections on Rights, Rituals and the Colored Conventions Project, by Carol A. Rudisell — Selected as Editors’ Choice by Digital Humanities Now\n",
            " \n",
            "\n",
            "\n",
            "Press, Academic Blogs and Websites\n",
            "\n",
            "CCP team, “Digitally Improving Historical Knowledge,” DML Central. Dec 25, 2017. \n",
            "Bond, Sarah E. “How Is Digital Mapping Changing The Way We Visualize Racism and Segregation?” Forbes. Oct. 20, 2017.  \n",
            "McGrath, James. “Put It In (Digital) Writing: Transcribing The Amazing Jobs of Frederick Douglass with The Colored Conventions Project.” Blog of the Center for Public Humanities and Cultural Heritage, Brown University. Feb. 22, 2017.\n",
            "Bies, Jessica. “UD group celebrates Frederick Douglass’ birthday.” The News Journal (DE), page 18. February 15, 2017.\n",
            "Hong, Albert. “University of Delaware’s Colored Conventions Project wins an MLA award.” Technical.ly Delaware. Dec. 13, 2016. \n",
            "Eric Ruth, Jim Casey, and P. Gabrielle Foreman. “Black Voices Arise from the Past.” University of Delaware Messenger, Vol. 24, No. 3. 1820.\n",
            "Rosinbum, John. “Uncovering Activism and Engaging Students: The Colored Conventions Project.” AHA Today: A Blog of the American Historical Association. Dec. 12, 2016. \n",
            "Coard, Michael. “Resurrect Philly’s 1830 Black economic, educational activism.” The Philadelphia Tribune. Sept. 17, 2016. \n",
            "Kahn, Eve. Colored Conventions, a Rallying Point for Black Americans Before the Civil War. New York Times. Aug. 4, 2016. \n",
            "Onion, Rebecca. Five More Digital History Projects We Loved in 2015. Slate Magazine. Dec. 21, 2015. \n",
            "Ashenfelder, Mike. “Cultural Institutions Embrace Crowdsourcing.” The Signal: Digital Preservation. Library of Congress, 16 Sept. 2015. Web.  \n",
            "Singh, Amardeep. “The Archive Gap: Race, the Canon, and the Digital Humanities.” Amardeep Singh, <http://www.electrostani.com> 14 Sept. 2015. Web.\n",
            "Hoffman, Anne and Karl Malgiero. “History Matters: Colored Conventions.” History Matters. Delaware Public Media, 1 May 2015. Web.\n",
            "“A Librarian’s Role in the Digital Humanities: The Colored Conventions Project” (featuring project member Carol Rudisell). Jottings & Digressions. University of Wisconsin School of Library and Information Studies, Spring 2015. Web.\n",
            "Fox, James. “Black Originalism Part 3: The Syracuse Convention of 1864.” The Faculty Lounge. Publisher, 21 March 2015. Web.\n",
            "Parasnis-Samar, Anjali. “Crowd-Sourced Project: 19th-Century ‘Colored Conventions.” Information Space. School of Information Studies, Syracuse University, 19 Feb 2015. Web.\n",
            "Fagan, Ben. “Colored Conventions and the Early Black Press.” Black Press Research Collective. Black Press Research Collective, 31 January 2015. Web.\n",
            "Gates, Henry Louis Jr. and Lindsay Fulton. “I’m Black, but I Want to Join the DAR. Help!” The Root, 7 March 2014. Web.\n",
            "\n",
            "\n",
            " UDaily\n",
            "\n",
            "Celebrating Frederick Douglass, Feb. 20 by Ann Manser.\n",
            "UD team working on Colored Conventions Project wins national award, Dec. 12, 2016. \n",
            "Accessible Archives New agreement will enhance research for Colored Conventions Project, May 11, 2016.\n",
            "NEH grant awarded to support Colored Conventions Project, April 14, 2016.\n",
            "UD Library announces historic agreement with Gale Cengage Learning, Colored Conventions Project, Sept 3, 2015.\n",
            "‘Colored Conventions’ symposium, April 20, 2015.\n",
            "‘Transcribe Minutes’ Colored Conventions Project launches crowd-sourcing initiative, March 20, 2015.\n",
            "\n",
            "\n",
            "The Colored Conventions Project, Douglass Day and the Black Women's Organizing Archive\n",
            "are flagship projects of the Center for Black Digital Research, #DigBlk, at Penn State University.\n",
            "\n",
            "\n",
            "The Colored Conventions Project appreciates the support of:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " \n",
            "The Colored Conventions Project was launched & cultivated at the University of Delaware from 2012-2020.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get Colored Conventions' text: ccc_text\n",
        "ccc_text = soup.get_text()\n",
        "# Print CCC's text \n",
        "# print(ccc_text[:500])\n",
        "\n",
        "# to get only the citations select the divs with the class used for each set of citations\n",
        "mydivs = soup.find_all(\"div\", {\"class\": \"et_pb_text\"})\n",
        "for div in mydivs:\n",
        "    print(div.get_text())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "30cgMgnJI1d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/about/book/\n",
            "https://coloredconventions.org/\n",
            "https://coloredconventions.org/\n",
            "https://coloredconventions.org/about-conventions/\n",
            "https://coloredconventions.org/about-conventions/\n",
            "https://coloredconventions.org/about-records/\n",
            "https://coloredconventions.org/about-conventions/submit-records/\n",
            "https://coloredconventions.org/about-records/ccp-corpus/\n",
            "https://coloredconventions.org/bibliography/\n",
            "https://coloredconventions.org/exhibits/\n",
            "https://coloredconventions.org/teaching/\n",
            "https://coloredconventions.org/teaching/#teaching-partners\n",
            "https://coloredconventions.org/curriculum/\n",
            "https://coloredconventions.org/news/\n",
            "https://douglassday.org/\n",
            "https://douglassday.org/\n",
            "https://coloredconventions.org/digblk/symposium-ccp-making-social-movement/\n",
            "https://coloredconventions.org/news/mural-dedication-philadelphia/\n",
            "https://coloredconventions.org/news/\n",
            "https://coloredconventions.org/about/press-notices/\n",
            "https://coloredconventions.org/about/videos/\n",
            "https://coloredconventions.org/about/\n",
            "https://coloredconventions.org/about/principles/\n",
            "https://coloredconventions.org/about/team/\n",
            "https://coloredconventions.org/about/team/committees/\n",
            "https://coloredconventions.org/about/cv/\n",
            "https://coloredconventions.org/about/speakers-agreement/\n",
            "https://coloredconventions.org/digblk/\n",
            "https://coloredconventions.org/about/using-two-sites/\n",
            "https://coloredconventions.org/contact/\n",
            "https://coloredconventions.org/donate/\n",
            "http://muse.jhu.edu/journals/american_periodicals/v026/26.1.fagan.html\n",
            "http://muse.jhu.edu/login?auth=0&type=summary&url=/journals/early_american_literature/v051/51.1.spires.html\n",
            "https://muse.jhu.edu/login?auth=0&type=summary&url=/journals/american_periodicals/v025/25.2.gardner01.html\n",
            "https://muse.jhu.edu/article/593047\n",
            "http://common-place.org/article/column/colored-conventions-project/\n",
            "http://common-place.org/book/the-colored-conventions-project-and-the-changing-same/\n",
            "http://common-place.org/book/toward-meaning-making-in-the-digital-age-black-women-black-data-and-colored-conventions/\n",
            "http://common-place.org/book/the-colored-conventions-movement-in-print-and-beyond/\n",
            "http://common-place.org/book/convention-minutes-and-unconventional-proceedings/\n",
            "http://common-place.org/book/liberating-history-reflections-on-rights-rituals-and-the-colored-conventions-project/\n",
            "http://digitalhumanitiesnow.org/2016/03/digital-rudisell-of-the-colored-conventions-project-on-copyright-and-doing-digital-black-history/\n",
            "https://dmlcentral.net/digitally-improving-historical-knowledge/\n",
            "https://www.forbes.com/sites/drsarahbond/2017/10/20/how-is-digital-mapping-changing-the-way-we-visualize-racism-and-segregation/\n",
            "https://www.brown.edu/academics/public-humanities/news/2017-02/put-it-digital-writing-transcribing-amazing-jobs-frederick-douglass-colored-conventio-0%20\n",
            "http://www.delawareonline.com/story/news/education/2017/02/15/ud-group-celebrates-frederick-douglass-birthday/97948962/\n",
            "http://technical.ly/delaware/2016/12/13/colored-conventions-project-mla-award/\n",
            "http://www1.udel.edu/udmessenger/vol24no3/digital/vol24no3/index.html#p=20\n",
            "http://blog.historians.org/2016/12/uncovering-activism-engaging-students-colored-conventions-project/\n",
            "http://www.phillytrib.com/commentary/resurrect-philly-s-black-economic-educational-activism/article_27c207d4-c8e0-5f3a-87e9-67ee3cf9dbce.html\n",
            "http://www.nytimes.com/2016/08/05/arts/design/colored-conventions-a-rallying-point-for-black-americans-before-the-civil-war.html?_r=0\n",
            "http://www.slate.com/blogs/the_vault/2015/12/21/some_neat_new_digital_history_projects_that_were_new_in_2015.html\n",
            "http://blogs.loc.gov/digitalpreservation/2015/09/cultural-institutions-embrace-crowdsourcing/\n",
            "http://www.electrostani.com/2015/09/the-archive-gap-race-canon-and-digital.html\n",
            "http://delawarepublic.org/post/history-matters-colored-conventions\n",
            "https://slis.wisc.edu/wp-content/uploads/2016/02/2015_spring_jottings.pdf\n",
            "http://www.thefacultylounge.org/2015/03/black-originalism-part-3-the-syracuse-convention-of-1864.html\n",
            "https://web.archive.org/web/20160428054608/http://infospace.ischool.syr.edu/2015/02/19/crowd-sourced-project-19th-century-colored-conventions/\n",
            "http://blackpressresearchcollective.org/2015/01/31/colored-conventions-and-the-early-black-press-by-benjamin-fagan/\n",
            "https://www.theroot.com/i-m-black-but-i-want-to-join-the-dar-help-1790874869\n",
            "http://www.udel.edu/udaily/2017/february/frederick-douglass-historical-records-transcription/\n",
            "http://www.udel.edu/udaily/2016/december/mla-award-colored-conventions-project/\n",
            "http://www1.udel.edu/udaily/2016/may/colored-conventions-accessible-archives-051116.html\n",
            "http://www1.udel.edu/udaily/2016/apr/neh-colored-conventions-041416.html\n",
            "http://www.udel.edu/udaily/2016/sep/library-agreement-090315.html\n",
            "http://www1.udel.edu/udaily/2015/apr/colored-conventions-042015.html\n",
            "http://www1.udel.edu/udaily/2015/mar/transcribe-minutes-031015.html\n",
            "https://coloredconventions.org/\n",
            "https://douglassday.org/\n",
            "https://bwoaproject.org/\n",
            "https://digblk.psu.edu/\n",
            "https://www.facebook.com/ColoredConventionsProject/\n",
            "https://twitter.com/CCP_org\n",
            "https://www.youtube.com/channel/UCrZ2dCB7rELCkSwxf21wLRA\n"
          ]
        }
      ],
      "source": [
        "# Find all 'a' tags (which define hyperlinks): a_tags\n",
        "a_tags = soup.find_all('a')\n",
        "\n",
        "# Print the URLs to the shell\n",
        "for link in a_tags:\n",
        "    print(link.get('href')[0:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o2vTt-YKid0"
      },
      "source": [
        "# Questions for reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcayZQmI1d8"
      },
      "source": [
        "Explain what the value is of importing HTML files using BeautifulSoup. How does this relate to the concerns that Rawson and Muñoz raise in their article? Are there times when you might want to keep the HTML? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TH0pQLjJKc5C"
      },
      "source": [
        "BeatifulSoup provides a number of methods to interact with the HTML document's structure and extract particular pieces of text based on an understanding of HTML markup and it's usage. HTML tags are themselves a method for structuring text. Tags that wrap text H1, H2, H3, P, I, NAV, etc. all have meaning are are chosen to separate headings, paragraphs, lists, tables, and more. Preserving an understanding of which texts were structured with different tags could help keep clean which text elements are emphasized over others as well as the hierarchical structures used to structure the text in HTML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFdXpqJoJnJb"
      },
      "source": [
        "Consider Rob Kitchin's criteria of \"good data.\" Would these datasets satisfy his definition of \"good data\"? Why or why not? What kinds of questions could one ask about the Colored Conventions Project using what you've learned here? "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I believe Kitchin would be able to label webscraped data as 'good' if certain conditions were met. For example, if the process for making decisions on which information is pulled out from a webpage, how this happens, and what is left behind, I belive this would show the good data practice of being accountable in documenting the process. As well if it were possible if the is clearly named relationship between the webscraper and the creator of the webpages then this would likely be considered ethically and socially responsible. And if both sets of stakeholders with potentially different perspectives on what makes the data good is incorported in the history of the data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

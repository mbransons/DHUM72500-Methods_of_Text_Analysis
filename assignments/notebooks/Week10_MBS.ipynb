{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmrhody/femethodsS23/blob/main/Week10notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gUfYy6SOb9rl"
      },
      "source": [
        "# Week 10 Notebook\n",
        "\n",
        "Name: Michael Smith\n",
        "\n",
        "Date:\n",
        "\n",
        "Class: \n",
        "\n",
        "Notes: (Anything you'd like to share with me before I read your notebook)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3KkeKuUTcSdz"
      },
      "source": [
        "This week's notebook is based on Chapter 5 from \n",
        "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text) by\n",
        "Jens Albrecht, Sidharth Ramachandran, Christian Winkler. I've added cells to this week's notebook to help explain \n",
        "\n",
        "# Vectors, Features, and Similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb0Tnb1BFkQr"
      },
      "source": [
        "## Review: Preparing Data for Operationalization\n",
        "- Tokenizing: breaking documents into units (words, characters, sentences) called tokens. \n",
        " - Multiple tokens can be grouped together into n-grams. \n",
        "- Stemming: \n",
        " - Words = root + prefix, suffix\n",
        " - Root = where the core meaning is held\n",
        " - Stemming reduced words that have similar meanings but multiple forms, such as tense, plural, gerunds, etc. \n",
        "- Dimension Reduction\n",
        "  - Simplify the number of observations \n",
        "  - Stopword reduction (removing words that don't convey \"meaninig\") \n",
        "  - Remove words that are too frequent or too unique\n",
        "  - remove other terms that are likely to confuse the counting. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i5LPUIbcG6Ph"
      },
      "source": [
        "## Vectors\n",
        "Vectors are mathematical objects that encode length and direction (represents a position or change in a framework or space) A 1-dimensional array of numbers (components) is displayed as a distribution. When represented geometrically, vectors represent coordinates in an n-dimensional space where n is the number of dimensions (units being compared). \n",
        "- In machine learning, text is represented in an array of numbers\n",
        "- Natural extension of real numbers in mathematics is a tuple (pairs of two numbers) \n",
        "- Vectors are useful because they are numerical representations that are spatial and therefore have norms and distances\n",
        "- We use spatial properties to measure similarity\n",
        "- Measuring similarity is a fundamental principle for analyzing texts with computers\n",
        "- Occupying the same, or related space, represents similarities between vectors\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FmiG4-d4cSd2"
      },
      "source": [
        "# Feature Engineering and Syntactic Similarity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tf0t1VEcSd2"
      },
      "source": [
        "## Things to consider\n",
        "\n",
        "While this notebook follows Chapter 5 from *Blueprints for Text Analysis with Python*, I have also made some changes. Some changes reflect updates and modifications to arguments or methods in the code, while other changes are about trying to streamline and focus on the concept of vectorizing rather than looking too closely at computing power and resource management, which is something that you have to begin to take into consideration. For example, in the second half of the assignment, we'll be working with the [ABC News Headline dataset from Kaggle](https://www.kaggle.com/datasets/therohk/million-headlines) Rather than going into detail about how to complete the lemmatizing in batches, I truncate the dataset so that we can improve computing time. \n",
        "\n",
        "If preparing text corpora for machine learning is something that you are interested in doing, you'll want to spend some time learning about batch processing and maybe even working with a remote high power computing environment to improve the performance of the model. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxzIVYZcSd5"
      },
      "source": [
        "## Setup<div class='tocSkip'/>\n",
        "\n",
        "We are going to use the set up and directory locations from the book. If working on Google Colab: copy files and install required libraries. The following cell imports a github repository for chapter 5. The files are downloaded into the /content directory. During this phase, you're also installing the package Spacy. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s_ez2JUfcSd7"
      },
      "source": [
        "## Load Python Settings<div class=\"tocSkip\"/>\n",
        "\n",
        "Common imports, defaults for formatting in Matplotlib, Pandas etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# suppress warnings\n",
        "import warnings;\n",
        "warnings.filterwarnings('ignore');\n",
        "\n",
        "# common imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import glob\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import pprint as pp\n",
        "import textwrap\n",
        "import sqlite3\n",
        "import logging\n",
        "import gzip\n",
        "import csv\n",
        "\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "# register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n",
        "tqdm.pandas()\n",
        "\n",
        "# pandas display options\n",
        "# https://pandas.pydata.org/pandas-docs/stable/user_guide/options.html#available-options\n",
        "pd.options.display.max_columns = 30 # default 20\n",
        "pd.options.display.max_rows = 60 # default 60\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "# pd.options.display.precision = 2\n",
        "pd.options.display.max_colwidth = 200 # default 50; -1 = all\n",
        "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
        "pd.set_option('display.html.use_mathjax', False)\n",
        "\n",
        "# np.set_printoptions(edgeitems=3) # default 3\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plot_params = {'figure.figsize': (8, 4), \n",
        "               'axes.labelsize': 'large',\n",
        "               'axes.titlesize': 'large',\n",
        "               'xtick.labelsize': 'large',\n",
        "               'ytick.labelsize':'large',\n",
        "               'figure.dpi': 100}\n",
        "# adjust matplotlib defaults\n",
        "matplotlib.rcParams.update(plot_params)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PpBXXtbkcSd8"
      },
      "source": [
        "# Data preparation\n",
        "We begin by creating a very small corpus of sentences that come from the opening lines of *A Tale of Two Cities* by Charles Dickens. We're going to work on a very simplified version of vectorization. Our hope is that by working with this simplified example that it becomes easier to then extend that mental model to much larger tasks. \n",
        "\n",
        "The following cell takes the list of sentences and then tokenizes the words in each sentence. Next, we are going through each token in the list `sentences` and we are taking each word and putting it into a list that we store as `vocabulary`. Finally, we import Pandas and we use the `enumerate` function to create a list of tuples that include each word as a string and the position of the word in the list. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WBck02vTcSd8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['It', 0],\n",
              " ['was', 1],\n",
              " ['the', 2],\n",
              " ['best', 3],\n",
              " ['of', 4],\n",
              " ['times', 5],\n",
              " ['it', 6],\n",
              " ['worst', 7],\n",
              " ['age', 8],\n",
              " ['wisdom', 9],\n",
              " ['foolishness', 10]]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = [\"It was the best of times\", \n",
        "             \"it was the worst of times\", \n",
        "             \"it was the age of wisdom\", \n",
        "             \"it was the age of foolishness\"]\n",
        "\n",
        "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\n",
        "\n",
        "vocabulary = list(dict.fromkeys([w for s in tokenized_sentences for w in s]))\n",
        "\n",
        "import pandas as pd\n",
        "[[w, i] for i,w in enumerate(vocabulary)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j0bKs-hGcSd8"
      },
      "source": [
        "# One-hot by hand\n",
        "In the following cell, we're defining a function called onehot_encode. That function goes through the list of `tokenized_sentences` one word at a time and checks it against the vocabulary. If a word from the vocabulary exists in the tokenized sentence, then it places a 1 in a list. If the word from the vocabulary does not occur in the sentence, it places a 0. \n",
        "\n",
        "The result is a vector of binaries: ones and zeroes that represent the sentence by whether or not words in the vocabulary are present. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "scEWPZq_cSd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]: It was the best of times\n",
            "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0]: it was the worst of times\n",
            "[0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0]: it was the age of wisdom\n",
            "[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]: it was the age of foolishness\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def onehot_encode(tokenized_sentence):\n",
        "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]\n",
        "\n",
        "onehot = [onehot_encode(tokenized_sentence) for tokenized_sentence in tokenized_sentences]\n",
        "\n",
        "for (sentence, oh) in zip(sentences, onehot):\n",
        "    print(\"%s: %s\" % (oh, sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A3dz6PTiB7XT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is a sample vector created with a sentence that wasn't\n",
        "# included in the vocabulary list but has some of the words from the vocabulary.\n",
        "\n",
        "onehot_encode(\"the age of wisdom is the best of times\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MKwhi4HgCD45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If none of the words in a sentence appear in the vocabulary list, then\n",
        "# you will end up with a vector that has all null values. \n",
        "onehot_encode(\"John likes to watch movies. Mary likes to watch movies too.\".split())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1bsdI5-4CQKV"
      },
      "source": [
        "## The Document-Term Matrix\n",
        "The document term matrix is the vector representation of all documents adn is the most basic building block for nearly all machine learning tasks we will do this semester. Each sentence from the list of sentences is represented in a row. Each of the words in the vocabulary is represented in the columns. The following cell takes the one_hot encoding and turns it into a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "trtMKPDCBR2_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>It</th>\n",
              "      <th>was</th>\n",
              "      <th>the</th>\n",
              "      <th>best</th>\n",
              "      <th>of</th>\n",
              "      <th>times</th>\n",
              "      <th>it</th>\n",
              "      <th>worst</th>\n",
              "      <th>age</th>\n",
              "      <th>wisdom</th>\n",
              "      <th>foolishness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   It  was  the  best  of  times  it  worst  age  wisdom  foolishness\n",
              "0   1    1    1     1   1      1   0      0    0       0            0\n",
              "1   0    1    1     0   1      1   1      1    0       0            0\n",
              "2   0    1    1     0   1      0   1      0    1       1            0\n",
              "3   0    1    1     0   1      0   1      0    1       0            1"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(onehot, columns=vocabulary)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WV-dNAMT0u2J"
      },
      "source": [
        "### Calculating similarities\n",
        "Calculating the similarities between documents works by calculating the number of common 1s at the corresponding positions. In one-hot encoding, this is an extremely fast operation, as it can be calculated on the bit level by ANDing the vectors and counting the number of 1s in the resulting vectors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "btvNjODNcSd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the similarities between the first 2 sentences\n",
        "# the result is the number of 1s that are shared between the 2 sentences. \n",
        "sim = [onehot[0][i] & onehot[1][i] for i in range(0, len(vocabulary))]\n",
        "sum(sim)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hiGtORmwDPwn"
      },
      "source": [
        "## Scalar Product or Dot Product\n",
        "calculated by multiplying corresponding components of the two vectors and adding up the products. If a product can only be 1 if both factors are 1, we can calculate the number of common 1s in the vectors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AWo_4mkLcSd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot[0], onehot[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vbSkMKCgcSd-"
      },
      "source": [
        "## Out of vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AdOOgKCMcSd-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehot_encode(\"the age of wisdom is the best of times\".split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EixfjMIbcSd-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehot_encode(\"John likes to watch movies. Mary likes movies too.\".split())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tK7XTAyxcSd-"
      },
      "source": [
        "## document term matrix\n",
        "A matrix is a vector of vectors. In other words, each of the sentences is represented by a vector of 0s and 1s that represent whether or not a word in the vocabulary appears in the sentence. When you take all four vectors and put them in a list, it becomes a matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9XcX-mCbcSd-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
              " [0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
              " [0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0],\n",
              " [0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1]]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YtnfMq33cSd-"
      },
      "source": [
        "## Similarities\n",
        "\n",
        "We can also create a matrix of the similarity scores. So, in the following matrix example, we calculate the scalar or dot product similarity of each sentence compared to each of the other sentences in the corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aqiS-KFScSd-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[6, 4, 3, 3],\n",
              "       [4, 6, 4, 4],\n",
              "       [3, 4, 6, 5],\n",
              "       [3, 4, 5, 6]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.dot(onehot, np.transpose(onehot))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xX3VrQ4EcSd_"
      },
      "source": [
        "## Scikit learn one-hot vectorization\n",
        "\n",
        "We did onehot vectorization by hand, but you can also use a tool like scikit learn to do it. Scikit Learn's OneHotEncoder is designed for the specific purpose of categorizing features (it encodes the features into the data), and that's not what we're doing here. We just want to see the encoding in action, so we use the MultiLabelBinarizer (because we want to make it as complicated as possible...). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T01OoDOmcSd_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
              "       [0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0],\n",
              "       [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "lb = MultiLabelBinarizer()\n",
        "lb.fit([vocabulary])\n",
        "lb.transform(tokenized_sentences)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LZvbaevMmDrA"
      },
      "source": [
        "### Note: \n",
        "Vectorizing has two different methods: fit and transform. Fit *learns* the vocabulary. Transform *converts* the documents into vectors. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7lJ7Y1VzeiUj"
      },
      "source": [
        "# Text Statistics\n",
        "Counting words is the simplest approach to analyzing language and text. While language is fluid and dynamic, there are some predictable elements. For example, we can count on the fact that there are words that are frequently found across all documents and all language domains (even poetry). The most common words used in English documents are: the, of, to, and, in, is, for, The, that, and said. Inversely, rare words or words that appear only one time in a corpus are also very common and can comprise about 1/2 of the total words. These are *hapax legomena* (words that appear only once). \n",
        "\n",
        "Two \"laws\" describe this phenomeonon in text analysis: [*Zipf's law*](https://en.wikipedia.org/wiki/Zipf%27s_law) and [*Heaps' Law*](https://en.wikipedia.org/wiki/Heaps%27_law#:~:text=Heaps'%20law%20means%20that%20as,the%20distinct%20terms%20are%20drawn). \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajsfdl08mg8S"
      },
      "source": [
        "# Bag of Words Models\n",
        "Bag-of-words representations create vectors for documents that also preserve the frequency of words that appear in each document as a feature. The frequency of the words are used as part of the weighting of the model. Models like Latent Dirichlet Allocation (LDA) explicitly require a BoW approach. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tphaAYI7cSd_"
      },
      "source": [
        "# CountVectorizer\n",
        "Creating vectorizers from scratch can be very time intensive, and since a similar method can be repurposed for several types of modeling, we can use the same algorithm over and over again. If we use scikit-learn, that algorithm is part of the class called CountVectorizer. The process of turning documents into vectors is also called feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nxloZUJQcSd_"
      },
      "outputs": [],
      "source": [
        "#import sklearn's CountVectorizer and then rename the function as cv\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Y_x-P5pVcSd_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It was the best of times</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it was the worst of times</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it was the age of wisdom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it was the age of foolishness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>John likes to watch movies. Mary likes movies too.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Mary also likes to watch football games.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    0\n",
              "0                            It was the best of times\n",
              "1                           it was the worst of times\n",
              "2                            it was the age of wisdom\n",
              "3                       it was the age of foolishness\n",
              "4  John likes to watch movies. Mary likes movies too.\n",
              "5            Mary also likes to watch football games."
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "more_sentences = sentences + [\"John likes to watch movies. Mary likes movies too.\",\n",
        "                              \"Mary also likes to watch football games.\"]\n",
        "pd.DataFrame(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZKHNFs2WcSd_"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use the CountVectorizer to learn the new vocabulary in more_sentences\n",
        "cv.fit(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YGMnOjRucSd_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['age' 'also' 'best' 'foolishness' 'football' 'games' 'it' 'john' 'likes'\n",
            " 'mary' 'movies' 'of' 'the' 'times' 'to' 'too' 'was' 'watch' 'wisdom'\n",
            " 'worst']\n"
          ]
        }
      ],
      "source": [
        "# then print out the whole vocabulary\n",
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DlYeJuFkcSd_"
      },
      "outputs": [],
      "source": [
        "# then we use transform to vectorize all the sentences in the more_sentences variable \n",
        "dt = cv.transform(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fRBXyJLwcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<6x20 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 38 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# when we call the vairable dt, it will describe the matrix of vectors that CountVectorizer produced\n",
        "# that matrix is a vector of vectors\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uKtDVAlwcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>also</th>\n",
              "      <th>best</th>\n",
              "      <th>foolishness</th>\n",
              "      <th>football</th>\n",
              "      <th>games</th>\n",
              "      <th>it</th>\n",
              "      <th>john</th>\n",
              "      <th>likes</th>\n",
              "      <th>mary</th>\n",
              "      <th>movies</th>\n",
              "      <th>of</th>\n",
              "      <th>the</th>\n",
              "      <th>times</th>\n",
              "      <th>to</th>\n",
              "      <th>too</th>\n",
              "      <th>was</th>\n",
              "      <th>watch</th>\n",
              "      <th>wisdom</th>\n",
              "      <th>worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  also  best  foolishness  football  games  it  john  likes  mary   \n",
              "0    0     0     1            0         0      0   1     0      0     0  \\\n",
              "1    0     0     0            0         0      0   1     0      0     0   \n",
              "2    1     0     0            0         0      0   1     0      0     0   \n",
              "3    1     0     0            1         0      0   1     0      0     0   \n",
              "4    0     0     0            0         0      0   0     1      2     1   \n",
              "5    0     1     0            0         1      1   0     0      1     1   \n",
              "\n",
              "   movies  of  the  times  to  too  was  watch  wisdom  worst  \n",
              "0       0   1    1      1   0    0    1      0       0      0  \n",
              "1       0   1    1      1   0    0    1      0       0      1  \n",
              "2       0   1    1      0   0    0    1      0       1      0  \n",
              "3       0   1    1      0   0    0    1      0       0      0  \n",
              "4       2   0    0      0   1    1    0      1       0      0  \n",
              "5       0   0    0      0   1    0    0      1       0      0  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When we turn the vector of vectors (or matrix) into a dataframe, \n",
        "# we can see the features for each sentence \n",
        "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AAYbEzKnqbvC"
      },
      "source": [
        "## Calculating Cosign Similarities\n",
        "If you want to find similarities between documents, the trick is harder than just finding the 1s. The number of occurrences of each word can be greater, and that needs to be given additional weight in our calculation of similarity. We can use the angle between two vectors to measure the similarity between them. The output is limited to numbers between 0 and 1 with 0 representing no similarity and 1 representing exact similarity.  Scikit-learn has a function that helps us to do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EXKxWJ39cSeA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.83333333]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import cosine_similarity and then use it to calculate the angle between the first and second sentences.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity(dt[0], dt[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Is8-XdIwcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the more_sentences variable has 6 sentences (or docs), so we will need to compare each of the\n",
        "# six sentences to each of the other sentences. \n",
        "len(more_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f6yBWpbHcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.83</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5\n",
              "0 1.00 0.83 0.67 0.67 0.00 0.00\n",
              "1 0.83 1.00 0.67 0.67 0.00 0.00\n",
              "2 0.67 0.67 1.00 0.83 0.00 0.00\n",
              "3 0.67 0.67 0.83 1.00 0.00 0.00\n",
              "4 0.00 0.00 0.00 0.00 1.00 0.52\n",
              "5 0.00 0.00 0.00 0.00 0.52 1.00"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Each sentence is a row and a column. The numbers they share are their calculated\n",
        "# similarity. Obviously, document 1 and document 1 are the same, so their cosine \n",
        "# similarity score is 1. The added sentences have no overlap with the first 4, so \n",
        "# their cosine similarity is 0. \n",
        "pd.DataFrame(cosine_similarity(dt, dt))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wr_lLojctv6G"
      },
      "source": [
        "## Review\n",
        "Vectorizing textual data turns strings of vocabulary into a numerical array, which also becomes a set of features that we can use to compare one document to another. One way to do this is with the Bag-of-Words approach. Bag-of-words vectorizing does not take the order of the vocabulary into account, but it does take the frequency that a word appears in a document and gives that term higher weight. \n",
        "\n",
        "In the next section, we'll work with TF-IDF vectorizing which essentially \"punishes\" words that appear too frequently in the corpus. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "860VEIStcSeA"
      },
      "source": [
        "# TF/IDF\n",
        "\n",
        "Term Frequency - Inverse Document Frequency (tf-idf) counts the number of total word occurrences in the corpus in addition to the occurrences in a single document. It uses the relationship between the frequency that a term is used in a document and compares it to the frequency that it appears in the entire collection so that words that are frequently found in both one document and in all documents is then inversely weighted overall. \n",
        "\n",
        "The logic behind this approach is that if there are words that are used frequently in every document, then they are not likely to convey important information. Instead, important information will likely be found in uncommon words that convey something different. \n",
        "\n",
        "Inverted document frequency creates a penalty for common words. In fact, we can arrive at a tf-idf weighting even if we start with a Bag-of-Words matrix. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KQU0w5--cSeA"
      },
      "outputs": [],
      "source": [
        "# from the feature_extraction text package in sklearn, we import TfidfTransformer\n",
        "# then we fit and transform the variable dt from above. \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_dt = tfidf.fit_transform(dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "oKUk4fArcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>also</th>\n",
              "      <th>best</th>\n",
              "      <th>foolishness</th>\n",
              "      <th>football</th>\n",
              "      <th>games</th>\n",
              "      <th>it</th>\n",
              "      <th>john</th>\n",
              "      <th>likes</th>\n",
              "      <th>mary</th>\n",
              "      <th>movies</th>\n",
              "      <th>of</th>\n",
              "      <th>the</th>\n",
              "      <th>times</th>\n",
              "      <th>to</th>\n",
              "      <th>too</th>\n",
              "      <th>was</th>\n",
              "      <th>watch</th>\n",
              "      <th>wisdom</th>\n",
              "      <th>worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  also  best  foolishness  football  games   it  john  likes  mary   \n",
              "0 0.00  0.00  0.57         0.00      0.00   0.00 0.34  0.00   0.00  0.00  \\\n",
              "1 0.00  0.00  0.00         0.00      0.00   0.00 0.34  0.00   0.00  0.00   \n",
              "2 0.47  0.00  0.00         0.00      0.00   0.00 0.34  0.00   0.00  0.00   \n",
              "3 0.47  0.00  0.00         0.57      0.00   0.00 0.34  0.00   0.00  0.00   \n",
              "4 0.00  0.00  0.00         0.00      0.00   0.00 0.00  0.31   0.50  0.25   \n",
              "5 0.00  0.42  0.00         0.00      0.42   0.42 0.00  0.00   0.34  0.34   \n",
              "\n",
              "   movies   of  the  times   to  too  was  watch  wisdom  worst  \n",
              "0    0.00 0.34 0.34   0.47 0.00 0.00 0.34   0.00    0.00   0.00  \n",
              "1    0.00 0.34 0.34   0.47 0.00 0.00 0.34   0.00    0.00   0.57  \n",
              "2    0.00 0.34 0.34   0.00 0.00 0.00 0.34   0.00    0.57   0.00  \n",
              "3    0.00 0.34 0.34   0.00 0.00 0.00 0.34   0.00    0.00   0.00  \n",
              "4    0.61 0.00 0.00   0.00 0.25 0.31 0.00   0.25    0.00   0.00  \n",
              "5    0.00 0.00 0.00   0.00 0.34 0.00 0.00   0.34    0.00   0.00  "
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# next, we turn the matrix into a dataframe so we can see it. \n",
        "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "w5-i2NEVcSeA"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.68</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.46</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2    3    4    5\n",
              "0 1.00 0.68 0.46 0.46 0.00 0.00\n",
              "1 0.68 1.00 0.46 0.46 0.00 0.00\n",
              "2 0.46 0.46 1.00 0.68 0.00 0.00\n",
              "3 0.46 0.46 0.68 1.00 0.00 0.00\n",
              "4 0.00 0.00 0.00 0.00 1.00 0.43\n",
              "5 0.00 0.00 0.00 0.00 0.43 1.00"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can do the same cosine similarity calculation on the tf-idf matrix\n",
        "pd.DataFrame(cosine_similarity(tfidf_dt, tfidf_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2DmFRn84cSeB"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>aba decides against community broadcasting licence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  publish_date                                       headline_text\n",
              "0   2003-02-19  aba decides against community broadcasting licence\n",
              "1   2003-02-19      act fire witnesses must be aware of defamation\n",
              "2   2003-02-19      a g calls for infrastructure protection summit\n",
              "3   2003-02-19            air nz staff in aust strike for pay rise\n",
              "4   2003-02-19       air nz strike to affect australian travellers"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's look at a more \"real world\" example. The ABC News headline\n",
        "# dataset are headlines from the Australian news outlet from 2013-2017.\n",
        "# We read in the files as a csv, convert it to a dataframe and print the first few entries. \n",
        "\n",
        "file_path = 'data/abcnews/abcnews-date-text.csv.gz'\n",
        "with gzip.open(file_path, 'rt') as ABCNEWS_FILE:\n",
        "    headlines = pd.read_csv(ABCNEWS_FILE, parse_dates=[\"publish_date\"])\n",
        "\n",
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "J6OMkTca3k8I"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1103663"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's see how many rows the dataset has. \n",
        "len(headlines.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "dgFPPrl0cSeB"
      },
      "outputs": [],
      "source": [
        "# That's a lot of rows! We can't possibly do this by hand. \n",
        "# The following tkaes the sklearn tfidf vectorizer, learns the vocabulary, then \n",
        "# builds the vectors for each headline. \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "yW1yyRECcSeB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x95878 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 7001357 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# If we look at a description of the new dataframe, we see that the size of the \n",
        "# dataframe becomes enormous and most of the positions in the dataframe are \n",
        "# null values, which makes it very sparse.\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "a4n3YGjxcSeB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "56010856"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt.data.nbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "b5xJ2vtOcSeB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 1.        , 0.16913596,\n",
              "        0.16792138],\n",
              "       [0.        , 0.        , 0.        , ..., 0.16913596, 1.        ,\n",
              "        0.33258708],\n",
              "       [0.        , 0.        , 0.        , ..., 0.16792138, 0.33258708,\n",
              "        1.        ]])"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# One of the biggest challenges we face because of the size of the dataframe\n",
        "# is the amount of time that it would take to calculate similarity scores. \n",
        "# The following just calculates it for the first 10,000 rows. \n",
        "# %%time\n",
        "cosine_similarity(dt[0:10000], dt[0:10000])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_ispAR1cSeB"
      },
      "source": [
        "# Dimension Reduction\n",
        "In order to make the operation / model run more efficiently, we need to reduce the number of dimensions (vector spaces) in the dataset. We can do this by removing stopwords using a set vocabulary list, removing the most frequent and infrequently used words, and combining words by reducing them just to their word roots so that the vocabulary is smaller. That's what the next few cells will do. \n",
        "\n",
        "## Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "l3PAlE-IcSeB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "326\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<1103663x95588 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5616010 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "print(len(stopwords))\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "44dpHDkfcSeB"
      },
      "source": [
        "## min_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "sZ1rYnigcSeB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x58516 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5578938 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bynyGJK-cSeC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x6767 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 4788559 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=.0001)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bCIjlcFhcSeC"
      },
      "source": [
        "## max_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "0RzR7DLmcSeC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x95588 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5616010 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "2pf3w0jncSeC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x95875 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 6532752 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(max_df=0.1)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pUz7ForecSeC"
      },
      "source": [
        "## n-grams\n",
        "You are not limited to just looking at the similarity between single words. You can calculate the similarities between 1, 2, and 3 words that appear together. That will create a bit more contextual information. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "k-7mt_sLcSeC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1103663, 557787)\n",
            "66868960\n",
            "(1103663, 742391)\n",
            "71782784\n"
          ]
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,3), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"headline_text\"])\n",
        "print(dt.shape)\n",
        "print(dt.data.nbytes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "06Dr9K69cSeC"
      },
      "source": [
        "## Lemmas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-ldTRyOZcSeC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1103663/1103663 [1:10:16<00:00, 261.76it/s]  \n"
          ]
        }
      ],
      "source": [
        "# The following example cuts the dataset down considerably. We're only running the \n",
        "# lemmatizing on the first 25 headlines. It took about 3 hours to run the entire set\n",
        "# through the lemmatizer. Some of the values that follow will be incomplete, but it \n",
        "# should still give you a sense of how it all works. \n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nouns_adjectives_verbs = [\"NOUN\", \"PROPN\", \"ADJ\", \"ADV\", \"VERB\"]\n",
        "\n",
        "# for i, row in tqdm(headlines.iterrows(), total=len(headlines)):\n",
        "for i, row in tqdm(headlines.iterrows(), total=len(headlines)):\n",
        "    doc = nlp(str(row[\"headline_text\"]))\n",
        "    headlines.at[i, \"lemmas\"] = \" \".join([token.lemma_ for token in doc])\n",
        "    headlines.at[i, \"nav\"] = \" \".join([token.lemma_ for token in doc if token.pos_ in nouns_adjectives_verbs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "6MvZEW98cSeC"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>nav</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>aba decides against community broadcasting licence</td>\n",
              "      <td>aba decide against community broadcasting licence</td>\n",
              "      <td>aba decide community broadcasting licence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "      <td>act fire witness must be aware of defamation</td>\n",
              "      <td>act fire witness aware defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "      <td>a g call for infrastructure protection summit</td>\n",
              "      <td>g call infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "      <td>air nz staff aust strike pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "      <td>air nz strike to affect australian traveller</td>\n",
              "      <td>air nz strike affect australian traveller</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  publish_date                                       headline_text   \n",
              "0   2003-02-19  aba decides against community broadcasting licence  \\\n",
              "1   2003-02-19      act fire witnesses must be aware of defamation   \n",
              "2   2003-02-19      a g calls for infrastructure protection summit   \n",
              "3   2003-02-19            air nz staff in aust strike for pay rise   \n",
              "4   2003-02-19       air nz strike to affect australian travellers   \n",
              "\n",
              "                                              lemmas   \n",
              "0  aba decide against community broadcasting licence  \\\n",
              "1       act fire witness must be aware of defamation   \n",
              "2      a g call for infrastructure protection summit   \n",
              "3           air nz staff in aust strike for pay rise   \n",
              "4       air nz strike to affect australian traveller   \n",
              "\n",
              "                                         nav  \n",
              "0  aba decide community broadcasting licence  \n",
              "1          act fire witness aware defamation  \n",
              "2    g call infrastructure protection summit  \n",
              "3          air nz staff aust strike pay rise  \n",
              "4  air nz strike affect australian traveller  "
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headlines.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "yVuKZkdVcSeC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x82749 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5532227 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "VGh5mjJIcSeD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x79745 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5429758 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S3pDzR3JcSeD"
      },
      "source": [
        "## Remove top 10,000\n",
        "We could also use other dictionaries to remove common words in English. In the following example, we use a list from Google that is very popular of the most common words. Using a much larger list like this one will decrease the size of the dataframe to a more manageable size. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "n_kyPTSicSeD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x72274 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1357383 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_10000 = pd.read_csv(\"https://raw.githubusercontent.com/first20hours/google-10000-english/master/google-10000-english.txt\", header=None)\n",
        "tfidf = TfidfVectorizer(stop_words=list(set(top_10000.iloc[:,0].values)))\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "sBWVPRDYcSeD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x94689 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 1531237 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=list(set(top_10000.iloc[:,0].values)), min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"nav\"].map(str))\n",
        "dt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pABdRtXmcSeD"
      },
      "source": [
        "## Finding document most similar to made-up document\n",
        "\n",
        "Using this process, we can do something similar to what we did with the one hot encoding above. We can take a sentence that is not included in the vocabulary, vectorize it, and then calculate how similar it is to sentences in the existing corpus. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "kGgBFlwfcSeD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<1103663x48731 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 5498209 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf = TfidfVectorizer(stop_words=\"english\", min_df=2)\n",
        "dt = tfidf.fit_transform(headlines[\"lemmas\"].map(str))\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "cVGBKTiecSeD"
      },
      "outputs": [],
      "source": [
        "made_up = tfidf.transform([\"australia and new zealand discuss optimal apple size\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "QpsDgrLucSeD"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(made_up, dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "R-EMCtPucSeD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , ..., 0.08594098, 0.        ,\n",
              "       0.        ])"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sim[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KKM56zoBcSeE"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe tex2jax_ignore\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>lemmas</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>633391</th>\n",
              "      <td>2011-08-17</td>\n",
              "      <td>new zealand apple import</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633392</th>\n",
              "      <td>2011-08-17</td>\n",
              "      <td>new zealand apple import</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896666</th>\n",
              "      <td>2014-08-12</td>\n",
              "      <td>why size matter for apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633393</th>\n",
              "      <td>2011-08-17</td>\n",
              "      <td>new zealand apple industry hurt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633559</th>\n",
              "      <td>2011-08-18</td>\n",
              "      <td>federal push to ban new zealand apple</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       publish_date                                 lemmas\n",
              "633391   2011-08-17               new zealand apple import\n",
              "633392   2011-08-17               new zealand apple import\n",
              "896666   2014-08-12              why size matter for apple\n",
              "633393   2011-08-17        new zealand apple industry hurt\n",
              "633559   2011-08-18  federal push to ban new zealand apple"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headlines.iloc[np.argsort(sim[0])[::-1][0:5]][[\"publish_date\", \"lemmas\"]]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MGafTjcecSeF"
      },
      "source": [
        "# Finding most related words\n",
        "\n",
        "We can go through the headlines now and estimate which words in the headlines are most similar to other words in headlines. In this case, we're using cosine similarity distributions to compare words that are most likely to be found in similar contexts as other words. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "mxkynwfgcSeF"
      },
      "outputs": [],
      "source": [
        "tfidf_word = TfidfVectorizer(stop_words=\"english\", min_df=1000)\n",
        "dt_word = tfidf_word.fit_transform(headlines[\"headline_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "8FSgnSxscSeF"
      },
      "outputs": [],
      "source": [
        "r = cosine_similarity(dt_word.T, dt_word.T)\n",
        "np.fill_diagonal(r, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "_4ZDLygScSeG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"sri\" related to \"lanka\"\n",
            "\"hour\" related to \"country\"\n",
            "\"seekers\" related to \"asylum\"\n",
            "\"springs\" related to \"alice\"\n",
            "\"pleads\" related to \"guilty\"\n",
            "\"hill\" related to \"broken\"\n",
            "\"trump\" related to \"donald\"\n",
            "\"violence\" related to \"domestic\"\n",
            "\"climate\" related to \"change\"\n",
            "\"driving\" related to \"drink\"\n",
            "\"care\" related to \"aged\"\n",
            "\"gold\" related to \"coast\"\n",
            "\"royal\" related to \"commission\"\n",
            "\"mental\" related to \"health\"\n",
            "\"wind\" related to \"farm\"\n",
            "\"flu\" related to \"bird\"\n",
            "\"murray\" related to \"darling\"\n",
            "\"north\" related to \"korea\"\n",
            "\"hour\" related to \"2014\"\n",
            "\"world\" related to \"cup\"\n"
          ]
        }
      ],
      "source": [
        "voc = tfidf_word.get_feature_names_out()\n",
        "size = r.shape[0] # quadratic\n",
        "for index in np.argsort(r.flatten())[::-1][0:40]:\n",
        "    a = int(index/size)\n",
        "    b = index%size\n",
        "    if a > b:  # avoid repetitions\n",
        "        print('\"%s\" related to \"%s\"' % (voc[a], voc[b]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
